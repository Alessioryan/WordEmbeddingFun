{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (1.22.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scipy in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (from scipy) (1.22.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (3.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (from matplotlib) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sklearn in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (0.0.post1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (from scikit-learn) (1.22.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# REMEMBER TO ACTIVATE THE CONDA ENV FIRST\n",
    "\n",
    "! pip install numpy\n",
    "! pip install scipy\n",
    "! pip install matplotlib\n",
    "! pip install sklearn\n",
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/alessio/miniconda3/envs/mynlpenv/lib/python3.8/site-packages (4.65.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant components\n",
    "\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at how the data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "top_5000_common_words = []\n",
    "with open(\"glove.6B/glove.6B.300d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for index, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "        if index < 5000:\n",
    "            top_5000_common_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average length of a vector\n",
    "\n",
    "distance_sums = 0\n",
    "for word, embedding in embeddings_dict.items():\n",
    "    distance_sums += np.linalg.norm(embedding)\n",
    "average_distance = distance_sums / len(embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.474557736493945\n"
     ]
    }
   ],
   "source": [
    "print(average_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity\n",
    "\n",
    "def distance(word1, word2 = ''):\n",
    "    if not word2:\n",
    "        return np.linalg.norm(embeddings_dict[word1])\n",
    "    else:\n",
    "        return np.linalg.norm(embeddings_dict[word1] - embeddings_dict[word2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work with individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8580456\n",
      "4.866664\n",
      "2.6515272\n"
     ]
    }
   ],
   "source": [
    "print(distance('dog'))\n",
    "print(distance('dogs'))\n",
    "print(distance('dog', 'dogs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given two words, return the index where the two arrays are most different\n",
    "def max_difference(word1, word2):\n",
    "    return np.argmax(np.abs(embeddings_dict[word1] - embeddings_dict[word2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(30, 4105), (25, 2241), (31, 1639), (22, 1100), (42, 1069), (4, 953), (43, 910), (19, 895), (8, 868), (32, 811), (48, 782), (49, 758), (5, 724), (26, 709), (38, 680), (0, 676), (3, 664), (13, 649), (33, 624), (18, 621), (29, 600), (44, 587), (12, 583), (1, 579), (45, 564), (17, 554), (2, 522), (37, 507), (39, 502), (47, 501), (9, 492), (16, 483), (41, 462), (20, 456), (7, 449), (6, 433), (28, 430), (40, 412), (24, 391), (21, 390), (34, 376), (14, 349), (11, 335), (27, 331), (15, 318), (46, 279), (23, 275), (36, 266), (35, 235), (10, 215)]\n"
     ]
    }
   ],
   "source": [
    "biggest_difference = defaultdict(int)\n",
    "for word, embedding in embeddings_dict.items():\n",
    "    if (word + 's') in embeddings_dict:\n",
    "        biggest_difference[max_difference(word, word + 's')] += 1\n",
    "    elif (word + 'es') in embeddings_dict:\n",
    "        biggest_difference[max_difference(word, word + 'es')] += 1\n",
    "sorted_differences = sorted(biggest_difference.items(), key=(lambda x: x[1]), reverse=True)\n",
    "print(sorted_differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal n: 999.9999700496764\n",
      "Minimum loss: 29659.794795824222\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def loss(n):\n",
    "    # Calculate the loss for a given value of n\n",
    "    loss = 0\n",
    "    for i in range(1, 50):\n",
    "        predicted_value = n / i\n",
    "        actual_value = sorted_differences[i - 1][1]\n",
    "        loss += abs(predicted_value - actual_value)\n",
    "    return loss\n",
    "\n",
    "# Find the value of n that minimizes the loss\n",
    "result = minimize_scalar(loss, method='bounded', bounds=(0.1, 1000))\n",
    "\n",
    "print(f\"Optimal n: {result.x}\")\n",
    "print(f\"Minimum loss: {result.fun}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nothing here! \n",
    "\n",
    "Time to look at the values for each index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(30, 26727), (25, 25177), (31, 21030), (42, 19360), (43, 19345), (22, 19150), (4, 19105), (19, 18874), (8, 18416), (49, 18328), (48, 18106), (5, 18094), (32, 18085), (33, 18074), (38, 18065), (18, 18011), (0, 17963), (3, 17851), (12, 17785), (26, 17750), (1, 17690), (29, 17609), (13, 17595), (17, 17592), (44, 17337), (41, 17279), (39, 17269), (45, 17263), (47, 17196), (2, 17056), (6, 17047), (37, 16940), (7, 16861), (20, 16791), (16, 16673), (40, 16647), (28, 16627), (9, 16557), (14, 16517), (24, 16505), (21, 16337), (34, 16170), (27, 15923), (11, 15780), (15, 15609), (35, 15575), (46, 15470), (23, 15398), (36, 15313), (10, 15049)]\n"
     ]
    }
   ],
   "source": [
    "overall_differences_vector = np.zeros(embeddings_dict['the'].shape[0])\n",
    "for word, embedding in embeddings_dict.items():\n",
    "    if (word + 's') in embeddings_dict:\n",
    "        overall_differences_vector += np.abs(embeddings_dict[word] - embeddings_dict[word + 's'])\n",
    "    elif (word + 'es') in embeddings_dict:\n",
    "        overall_differences_vector += np.abs(embeddings_dict[word] - embeddings_dict[word + 'es'])\n",
    "reversed_indeces = np.argsort(overall_differences_vector)[::-1]\n",
    "print([(reversed_index, round(overall_differences_vector[reversed_index])) for reversed_index in reversed_indeces])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok still nothing :/\n",
    "\n",
    "But what about past tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likely_s_adding_method(word):\n",
    "    if word[-1] == 's' or word[-1] == 'z' or word[-1] == 'x' or (len(word) > 1 and (word[-2:] == 'sh' or word[-2:] == 'ch')):\n",
    "        return word + 'es'\n",
    "    return word + 's'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_probably_a_verb(word):\n",
    "    return word + 'ing' in embeddings_dict and word + 'ed' in embeddings_dict and likely_s_adding_method(word) in embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past:  [(30, 646), (32, 194), (31, 106), (49, 94), (26, 90), (1, 88), (46, 82), (27, 78), (6, 76), (37, 71), (25, 61), (18, 60), (21, 59), (16, 55), (12, 53), (47, 53), (38, 50), (15, 49), (19, 49), (4, 43), (28, 42), (11, 41), (3, 40), (22, 40), (44, 37), (9, 36), (29, 30), (7, 29), (13, 28), (20, 27), (8, 27), (5, 27), (0, 26), (39, 24), (43, 24), (23, 23), (45, 23), (48, 22), (36, 22), (2, 20), (33, 20), (40, 17), (14, 16), (34, 15), (42, 14), (17, 14), (24, 14), (35, 12), (10, 12), (41, 7)]\n",
      "present:  [(30, 624), (15, 155), (32, 133), (25, 113), (26, 85), (49, 76), (29, 70), (1, 68), (16, 68), (2, 65), (18, 58), (27, 56), (31, 55), (39, 53), (19, 51), (47, 51), (11, 50), (21, 49), (3, 41), (4, 41), (33, 41), (38, 40), (17, 38), (0, 38), (28, 37), (45, 37), (12, 36), (43, 36), (9, 34), (5, 32), (6, 30), (37, 29), (22, 28), (7, 28), (14, 27), (13, 27), (8, 26), (44, 25), (48, 24), (20, 24), (24, 24), (40, 24), (36, 23), (23, 21), (46, 20), (35, 19), (42, 17), (34, 17), (41, 13), (10, 9)]\n",
      "3sg:  [(30, 623), (25, 196), (32, 133), (31, 111), (19, 87), (26, 77), (43, 73), (22, 65), (8, 63), (49, 62), (4, 54), (33, 48), (15, 47), (44, 45), (42, 43), (38, 43), (1, 43), (5, 43), (27, 42), (18, 41), (39, 41), (0, 41), (21, 38), (48, 37), (37, 36), (16, 36), (20, 35), (47, 35), (28, 34), (17, 32), (11, 32), (29, 32), (13, 31), (6, 31), (12, 30), (3, 29), (40, 28), (45, 27), (23, 26), (2, 24), (35, 22), (24, 22), (7, 21), (46, 21), (14, 21), (41, 21), (36, 19), (10, 17), (9, 15), (34, 13)]\n"
     ]
    }
   ],
   "source": [
    "biggest_difference_past = defaultdict(int)\n",
    "biggest_difference_present = defaultdict(int)\n",
    "biggest_difference_3sg = defaultdict(int)\n",
    "for word, embedding in embeddings_dict.items():\n",
    "    if is_probably_a_verb(word):\n",
    "        biggest_difference_past[max_difference(word, word + 'ed')] += 1\n",
    "        biggest_difference_present[max_difference(word, word + 'ing')] += 1\n",
    "        biggest_difference_3sg[max_difference(word, likely_s_adding_method(word))] += 1\n",
    "sorted_differences_past = sorted(biggest_difference_past.items(), key=(lambda x: x[1]), reverse=True)\n",
    "sorted_differences_present = sorted(biggest_difference_present.items(), key=(lambda x: x[1]), reverse=True)\n",
    "sorted_differences_3sg = sorted(biggest_difference_3sg.items(), key=(lambda x: x[1]), reverse=True)\n",
    "print(\"past: \", sorted_differences_past)\n",
    "print(\"present: \", sorted_differences_present)\n",
    "print(\"3sg: \", sorted_differences_3sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok last thing before I sleep\n",
    "\n",
    "Is it just that 30 is use more often???? Idrk what this even means but does it really represent inflectional morphology?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "[(30, 664), (26, 321), (13, 312), (5, 302), (11, 300), (8, 297), (4, 260), (0, 248), (33, 247), (20, 247), (12, 230), (49, 224), (47, 222), (18, 222), (6, 220), (28, 219), (3, 218), (9, 211), (42, 205), (43, 204), (38, 202), (45, 199), (25, 199), (44, 197), (2, 194), (37, 193), (1, 192), (39, 191), (48, 186), (16, 182), (41, 179), (7, 174), (19, 173), (14, 165), (22, 161), (31, 159), (21, 155), (24, 152), (29, 145), (23, 144), (32, 143), (34, 138), (17, 129), (40, 113), (46, 108), (10, 108), (15, 105), (35, 85), (27, 81), (36, 75)]\n"
     ]
    }
   ],
   "source": [
    "biggest_random_difference = defaultdict(int)\n",
    "for i in range(10000):\n",
    "    random_keys = random.sample(embeddings_dict.keys(), 2)\n",
    "    biggest_random_difference[max_difference(random_keys[0], random_keys[1])] += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "sorted_random_differences = sorted(biggest_random_difference.items(), key=(lambda x: x[1]), reverse=True)\n",
    "print(sorted_random_differences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(30, 95612), (25, 32342), (6, 12237), (12, 10977), (26, 9630), (18, 9443), (1, 9009), (33, 8669), (5, 8478), (4, 8067), (8, 7457), (13, 7450), (11, 7249), (9, 7048), (14, 7015), (0, 6630), (43, 6203), (20, 6128), (45, 6061), (47, 5992), (49, 5683), (38, 5630), (28, 5598), (3, 5581), (19, 5549), (7, 5535), (41, 5388), (44, 5384), (42, 4843), (32, 4815), (24, 4650), (22, 4636), (2, 4607), (39, 4607), (37, 4595), (46, 4487), (48, 4352), (31, 4300), (16, 4245), (17, 4164), (21, 3930), (40, 3809), (29, 3570), (34, 3467), (23, 3269), (15, 2686), (27, 2685), (10, 2255), (36, 2011), (35, 1972)]\n"
     ]
    }
   ],
   "source": [
    "# Ok but is it just that it's the biggest????\n",
    "\n",
    "biggest_index = defaultdict(int)\n",
    "for word, embedding in embeddings_dict.items():\n",
    "    biggest_index[np.argmax(np.abs(embedding))] += 1\n",
    "sorted_biggest_indeces = sorted(biggest_index.items(), key=(lambda x: x[1]), reverse=True)\n",
    "print(sorted_biggest_indeces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok well now I need to redo it all but normalized this is some bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok clearly my earlier tests weren't working today is Dec 23 2023 and I want results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the similarity between two vectors as the cosine similarity\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "\n",
    "    similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most similar vector to another one\n",
    "def most_similar(raw_vector, embeddings_dict, top_k):\n",
    "    cosine_similarities = []\n",
    "    for word in embeddings_dict:\n",
    "        cosine_similarities.append((word, cosine_similarity(raw_vector, embeddings_dict[word] ) ) )\n",
    "    # Sort based on the cosine similarity\n",
    "    cosine_similarities = sorted(cosine_similarities, key=lambda x: x[1], reverse=True)\n",
    "    return cosine_similarities[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(vector):\n",
    "    return np.linalg.norm(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('test', 1.0), ('tests', 0.91643566), ('testing', 0.81992215), ('tested', 0.74428755), ('final', 0.69102275)]\n"
     ]
    }
   ],
   "source": [
    "print(most_similar(embeddings_dict[\"test\"], embeddings_dict, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tall - october + campaigns\n",
      "[('tall', 0.64718676), ('campaigns', 0.5039532), ('taller', 0.39721638), ('erect', 0.38956788), ('ads', 0.38947594)]\n"
     ]
    }
   ],
   "source": [
    "# Get three random words and operate on them. Then, find the closest words\n",
    "vector1 = random.choice(top_5000_common_words)\n",
    "vector2 = random.choice(top_5000_common_words)\n",
    "vector3 = random.choice(top_5000_common_words)\n",
    "print(f\"{vector1} - {vector2} + {vector3}\")\n",
    "most_similar_words = most_similar(embeddings_dict[vector1] - embeddings_dict[vector2] + embeddings_dict[vector3], embeddings_dict, 5)\n",
    "print(most_similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.8651047\n",
      "6.5954485\n",
      "6.8510113\n",
      "dance - dancer + sing\n",
      "[('sing', 0.808328), ('dance', 0.6101845), ('singing', 0.59580183), ('songs', 0.5877572), ('tune', 0.53933567)]\n"
     ]
    }
   ],
   "source": [
    "# Get three not random words and operate on them. Then, find the closest words\n",
    "# dance - dancer = sing - singer\n",
    "# dance - dancer - sing = -singer\n",
    "# -dance + dancer + sing = singer\n",
    "vector1 = \"dance\"\n",
    "vector2 = \"dancer\"\n",
    "vector3 = \"sing\"\n",
    "print(length(embeddings_dict[vector1]))\n",
    "print(length(embeddings_dict[vector2]))\n",
    "print(length(embeddings_dict[vector3]))\n",
    "print(f\"{vector1} - {vector2} + {vector3}\")\n",
    "most_similar_words = most_similar(embeddings_dict[vector1] - embeddings_dict[vector2] + embeddings_dict[vector3], embeddings_dict, 5)\n",
    "print(most_similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.8651047\n",
      "6.5954485\n",
      "6.8510113\n",
      "dance is to dancer as sing is to\n",
      "[('sing', 0.6812048), ('dancer', 0.55380404), ('sang', 0.53223425), ('sings', 0.5071834), ('singer', 0.49010777), ('baritone', 0.4507536), ('singers', 0.44889233), ('soprano', 0.44633728), ('singing', 0.44221634), ('nyah', 0.42972228)]\n"
     ]
    }
   ],
   "source": [
    "# Get three not random words and operate on them. Then, find the closest words\n",
    "# dance - dancer = sing - singer\n",
    "# dance - dancer - sing = -singer\n",
    "# -dance + dancer + sing = singer\n",
    "vector1 = \"dance\"\n",
    "vector2 = \"dancer\"\n",
    "vector3 = \"sing\"\n",
    "print(length(embeddings_dict[vector1]))\n",
    "print(length(embeddings_dict[vector2]))\n",
    "print(length(embeddings_dict[vector3]))\n",
    "print(f\"{vector1} is to {vector2} as {vector3} is to\")\n",
    "most_similar_words = most_similar(0 - embeddings_dict[vector1] + embeddings_dict[vector2] + embeddings_dict[vector3], embeddings_dict, 10)\n",
    "print(most_similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('mynlpenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac801eb61ad494b85a9603d25705671f410ddf7977290b6bffa25d9a89ed8809"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
